{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pettingzoo[classic]==1.22.3","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:48:02.570215Z","iopub.execute_input":"2023-09-06T21:48:02.572192Z","iopub.status.idle":"2023-09-06T21:48:12.404810Z","shell.execute_reply.started":"2023-09-06T21:48:02.572133Z","shell.execute_reply":"2023-09-06T21:48:12.403522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip '/content/to_upload.zip'","metadata":{"execution":{"iopub.status.busy":"2023-09-06T20:23:44.507631Z","iopub.execute_input":"2023-09-06T20:23:44.508011Z","iopub.status.idle":"2023-09-06T20:23:44.514611Z","shell.execute_reply.started":"2023-09-06T20:23:44.507978Z","shell.execute_reply":"2023-09-06T20:23:44.513302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport ray\nimport gymnasium as gym\nfrom gym.spaces import Box, Discrete\nfrom ray import tune\nfrom ray.rllib.algorithms.dqn import DQNConfig\nfrom ray.rllib.algorithms.dqn.dqn_torch_model import DQNTorchModel\nfrom ray.rllib.env import PettingZooEnv\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\nfrom ray.rllib.utils.framework import try_import_torch\nfrom ray.rllib.utils.torch_utils import FLOAT_MAX\nfrom ray.tune.registry import register_env\nfrom ray import air\nfrom pettingzoo.classic import hanabi_v4\n\ntorch, nn = try_import_torch()\n\n# Adapted from [1] and [2]\n\nif __name__ == \"__main__\":\n    ray.shutdown()\n    ray.init(log_to_driver=False, num_cpus = 4)\n\n    alg_name = \"DQN\"\n\n    def env_creator():\n        env = hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=2, max_information_tokens=3, max_life_tokens=1)\n        return env\n\n    env_name = \"hanabi_v4\"\n    register_env(env_name, lambda config: PettingZooEnv(env_creator()))\n\n    test_env = PettingZooEnv(env_creator())\n    obs_space = test_env.observation_space\n    act_space = test_env.action_space\n\n\n    config = (DQNConfig()\n        .environment(env=env_name)\n        .rollouts(num_rollout_workers=3, rollout_fragment_length=4)\n        .training(\n            noisy = True,\n            num_atoms = 51, # number of atoms for distributional DQN\n            train_batch_size=32, # batch size\n            hiddens=[], # Dense-layer setup for each the advantage branch and the value branch\n            dueling=False, # Whether to use dueling DQN.\n            gamma = 0.99, # discount factor\n            double_q = True,\n            model={\"fcnet_hiddens\": [512, 512], 'fcnet_activation': 'relu'}, # number of hidden nets and activation function\n            num_steps_sampled_before_learning_starts = 500,\n            target_network_update_freq= 500, # hard update of target network every 500 steps\n            replay_buffer_config = {\"capacity\": 1000000, \"prioritized_replay_alpha\": 0.5}, # prioritized replay buffer config\n            adam_epsilon = 0.00003125,  \n            n_step = 1 # n_step in n_step update\n\n        )\n        .multi_agent(\n            # set different policies for each player\n            policies={\n                \"player_0\": (None, obs_space, act_space, {}),\n                \"player_1\": (None, obs_space, act_space, {}),\n            },\n            policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n        )\n        .resources(num_gpus= 0)\n        .debugging(\n            log_level=\"DEBUG\"\n        )  \n        .framework(framework=\"torch\") # pytorch or tf\n        .exploration(\n            exploration_config={\n                # The Exploration class to use.\n                \"type\": \"EpsilonGreedy\",\n                # Config for the Exploration class' constructor:\n                \"initial_epsilon\": 0.000025,\n                \"final_epsilon\": 0.0,\n                \"epsilon_timesteps\": 1000,  # Timesteps over which to anneal epsilon.\n            }\n         ))\n#     .build())\n","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:48:12.540076Z","iopub.execute_input":"2023-09-06T21:48:12.540656Z","iopub.status.idle":"2023-09-06T21:48:20.030937Z","shell.execute_reply.started":"2023-09-06T21:48:12.540599Z","shell.execute_reply":"2023-09-06T21:48:20.029655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tune.run(\n#         alg_name,\n#         name=\"DQN\",\n#         restore = '/kaggle/input/checkpoint-20mil/kaggle_upload/checkpoint_019880',\n#         storage_path = '/kaggle/working/',\n#         stop={\"timesteps_total\": 22000000},\n#         checkpoint_freq=20,\n#         config=config.to_dict(),\n#     )","metadata":{"execution":{"iopub.status.busy":"2023-09-06T20:23:45.185725Z","iopub.status.idle":"2023-09-06T20:23:45.186097Z","shell.execute_reply.started":"2023-09-06T20:23:45.185911Z","shell.execute_reply":"2023-09-06T20:23:45.185930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r distributional_DQN_5.zip /root/ray_results","metadata":{"execution":{"iopub.status.busy":"2023-09-06T20:23:45.187509Z","iopub.status.idle":"2023-09-06T20:23:45.187920Z","shell.execute_reply.started":"2023-09-06T20:23:45.187716Z","shell.execute_reply":"2023-09-06T20:23:45.187736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ray.rllib.algorithms.algorithm import Algorithm\nfrom ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\nfrom ray.rllib.models import ModelCatalog\nfrom ray.tune.registry import register_env\nfrom pettingzoo.classic import hanabi_v4\n\n\nray.shutdown()\nray.init()\n\n\n\nalg_name = \"DQN\"\n\n\ndef env_creator():\n    env = hanabi_v4.env(colors=2, ranks=5, players=2, hand_size=2, max_information_tokens=3, max_life_tokens=1)\n    return env\n\n\nenv = env_creator()\nenv_name = \"hanabi_v4\"\nregister_env(env_name, lambda config: PettingZooEnv(env_creator()))\n\n# load trained DQN agent\nDQNAgent = Algorithm.from_checkpoint('/kaggle/input/checkpoint-20mil/kaggle_upload/checkpoint_019880')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:48:20.034602Z","iopub.execute_input":"2023-09-06T21:48:20.035000Z","iopub.status.idle":"2023-09-06T21:49:33.383577Z","shell.execute_reply.started":"2023-09-06T21:48:20.034966Z","shell.execute_reply":"2023-09-06T21:49:33.382343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch\nimport torch.nn as nn\n\n# the norm aware algorithm\n\nclass MultiOutputNormNet(torch.nn.Module):\n    def __init__(self, num_classes1, num_classes2):\n        super(MultiOutputNormNet, self).__init__()\n\n        self.fc1 = torch.nn.Linear(171, 256)\n        self.act = nn.ReLU()\n        self.fc2 = torch.nn.Linear(256, 512)\n        self.act2 = nn.ReLU()\n        self.fc21 = torch.nn.Linear(512, 256)\n        self.act3 = nn.ReLU()\n        self.dropout = nn.Dropout(0.20)\n        self.fc3 = torch.nn.Linear(256, num_classes1)\n        self.fc4 = torch.nn.Linear(256, num_classes2)\n        self.fc5 = torch.nn.Linear(256, num_classes2)\n        self.fc6 = torch.nn.Linear(256, num_classes2)\n\n    def forward(self, x):\n        output_1 = self.act(self.fc1(x))\n        output_2 = self.act2(self.fc2(output_1))\n        output_3 = self.act3(self.fc21(output_2))\n        output_3 = self.dropout(output_3)\n        output_4 = self.fc3(output_3)\n        output_5 = self.fc4(output_3)\n        output_6 = self.fc5(output_3)\n        output_7 = self.fc6(output_3)\n        return output_4, output_5, output_6, output_7","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:49:33.385659Z","iopub.execute_input":"2023-09-06T21:49:33.386327Z","iopub.status.idle":"2023-09-06T21:49:33.403290Z","shell.execute_reply.started":"2023-09-06T21:49:33.386221Z","shell.execute_reply":"2023-09-06T21:49:33.401664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MultiOutputNormNet(11, 3)\n# load trained norm aware algorithm\nmodel.load_state_dict(torch.load('/kaggle/input/data-checpoint/data_checkpoints/norm_learning_model.pth'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:49:33.404562Z","iopub.execute_input":"2023-09-06T21:49:33.405826Z","iopub.status.idle":"2023-09-06T21:49:33.436107Z","shell.execute_reply.started":"2023-09-06T21:49:33.405787Z","shell.execute_reply":"2023-09-06T21:49:33.434777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom ray.rllib.models.preprocessors import get_preprocessor\n\n\nlambda_val = 50\nepisodes = 1000\n\n\nreward_sums = {a: 0 for a in env.possible_agents}\n\n\n# episode = 0\nenv.reset()\n# iterate over episodes\nfor episode in range(1000):\n    \n        env.reset()\n        flag = False\n       \n        print('Episode',episode)\n        # iterate over agents\n        for agent in env.agent_iter():\n            # get environment observation\n            observation, reward, termination, truncation, info = env.last()\n            obs = observation[\"observation\"] # get the observation vector\n            reward_sums[agent] += reward # increment reward\n            \n            # if it is final state, no actions were taken\n            if termination or truncation:\n                action = None\n\n            else:\n                # set lambda_val 0 and 1 to different probabilities\n                lambda_val = np.random.choice(np.arange(0, 2), p=[0.0, 1.0])\n                \n                # output of norm aware algorithm\n                action_out, behav_norm, action_norm, state_norm = model(torch.from_numpy(observation['observation']))\n                action_out, behav_norm, action_norm, state_norm = model(torch.from_numpy(observation['observation']))\n                # get norm values\n                behav_norm = torch.argmax(behav_norm).cpu().detach().numpy()\n                action_norm = torch.argmax(action_norm).cpu().detach().numpy()\n                state_norm = torch.argmax(state_norm).cpu().detach().numpy()\n                \n                # take the actions according to output of norm aware algorithm if the actions are norms and lambda_val ==1\n                if lambda_val == 1 and (action_norm == 1 or behav_norm == 1 or state_norm == 1):\n                    action = torch.argmax(action_out).cpu().detach().numpy()\n                else:\n                    # otherwise select the best action\n                    prep = get_preprocessor(env.observation_space(agent))(env.observation_space(agent))\n                    # transform observation space to same configuration as the model\n                    p = prep.transform(observation)\n                    # get policy of current agent\n                    pol = config.to_dict()['multiagent']['policy_mapping_fn'](agent, _)\n                    policy = DQNAgent.get_policy(agent)\n                    # compute possible actions of agent\n                    out = policy.compute_actions_from_input_dict({\"obs\": p.reshape(1, -1)})\n                    # take the best action\n                    single_action = out[0]\n                    action = single_action[0] \n\n                    \n            env.step(action) \n\n            if termination or truncation:\n                if flag == True:\n                    print(\"reward_final :\", reward_sums) # print mean episode reward after each episodes\n                flag = True\n\n            env.render()\n\n            \nprint(\"episode reward mean :\",        sum(reward_sums.values() ) /episodes) # final mean episode reward","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:50:49.095123Z","iopub.execute_input":"2023-09-06T21:50:49.095555Z","iopub.status.idle":"2023-09-06T21:50:50.847243Z","shell.execute_reply.started":"2023-09-06T21:50:49.095519Z","shell.execute_reply":"2023-09-06T21:50:50.845628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-06T21:04:26.363983Z","iopub.execute_input":"2023-09-06T21:04:26.364464Z","iopub.status.idle":"2023-09-06T21:04:26.373223Z","shell.execute_reply.started":"2023-09-06T21:04:26.364413Z","shell.execute_reply":"2023-09-06T21:04:26.371493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n[1] “hanabi-learning-environment/hanabi_learning_environment/agents/rainbow/rainbow_agent.py at master · deepmind/hanabi-learning-environment,” GitHub. https://github.com/deepmind/hanabi-learning-environment/blob/master/hanabi_learning_environment/agents/rainbow/rainbow_agent.py (accessed Sep. 07, 2023).\n\n[2] “PettingZoo/tutorials/Ray/rllib_leduc_holdem.py at master · Farama-Foundation/PettingZoo,” GitHub. https://github.com/Farama-Foundation/PettingZoo/blob/master/tutorials/Ray/rllib_leduc_holdem.py (accessed Sep. 07, 2023).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}